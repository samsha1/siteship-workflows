name: Sync Airflow DAGs on Tag Push

on:
  push:
    tags:
      - 'v*'

jobs:
  sync-dags:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract tag
        id: vars
        run: echo "TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_ENV
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ghcr.io/${{ github.repository_owner }}/siteship-workflows:latest
            ghcr.io/${{ github.repository_owner }}/siteship-workflows:${{ env.TAG }}
      - name: Archive DAGs and Plugins
        run: |
          FNAME="airflow_sync.tar.gz"
          tar -czf "$FNAME" dags pyproject.toml poetry.lock
      - name: Cleanup old archive on remote
        uses: appleboy/ssh-action@v1.2.0
        with:
          host: ${{ secrets.ELESTIO_HOST }}
          username: ${{ secrets.ELESTIO_USER }}
          key: ${{ secrets.ELESTIO_SSH_KEY }}
          script: |
            if [ -e /tmp/airflow_sync.tar.gz ]; then
              echo "Removing old /tmp/airflow_sync.tar.gz..."
              rm -rf /tmp/airflow_sync.tar.gz
            fi
      - name: Copy to Elestio Server
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.ELESTIO_HOST }}
          username: ${{ secrets.ELESTIO_USER }}
          key: ${{ secrets.ELESTIO_SSH_KEY }}
          source: "airflow_sync.tar.gz"
          target: "/tmp/"

      - name: SSH into Server and Update Airflow DAGs
        uses: appleboy/ssh-action@v1.2.0
        with:
          host: ${{ secrets.ELESTIO_HOST }}
          username: ${{ secrets.ELESTIO_USER }}
          key: ${{ secrets.ELESTIO_SSH_KEY }}
          script: |
            ARCHIVE="/tmp/airflow_sync.tar.gz"

            echo "Checking archive exists and is regular file: $ARCHIVE"
            if [ ! -f "$ARCHIVE" ]; then
              echo "ERROR: expected regular file $ARCHIVE but not found. Listing /tmp:"
              ls -la /tmp | sed -n '1,200p'
              exit 2
            fi

            cd /opt/app

            echo "Extracting new DAGs & plugins from $ARCHIVE..."
            tar -xzf "$ARCHIVE" -C .

            echo "Restarting Airflow core services..."
            docker compose restart airflow-scheduler airflow-worker airflow-triggerer airflow-dag-processor

            echo "Cleaning up archive..."x
            rm -f "$ARCHIVE"

            echo "âœ… DAGs synced and Airflow restarted!"
